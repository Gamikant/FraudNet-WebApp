{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import h5py\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(dev_path, oos_path, oot_path):\n",
    "    dev = pd.read_csv(dev_path)\n",
    "    oos = pd.read_csv(oos_path)\n",
    "    oot = pd.read_csv(oot_path)\n",
    "    return dev, oos, oot\n",
    "\n",
    "def standardize_data(dev, oos, oot):\n",
    "    scaler = StandardScaler()\n",
    "    dev_scaled = scaler.fit_transform(dev.drop(columns=['target']))\n",
    "    oos_scaled = scaler.transform(oos.drop(columns=['target']))\n",
    "    oot_scaled = scaler.transform(oot.drop(columns=['target']))\n",
    "    return dev_scaled, oos_scaled, oot_scaled, scaler\n",
    "\n",
    "def split_data(dev, oos):\n",
    "    dev_F = dev[dev['target'] == 1].drop(columns=['target'])\n",
    "    dev_NF = dev[dev['target'] == 0].drop(columns=['target'])\n",
    "    oos_F = oos[oos['target'] == 1].drop(columns=['target'])\n",
    "    oos_NF = oos[oos['target'] == 0].drop(columns=['target'])\n",
    "    return dev_F, dev_NF, oos_F, oos_NF\n",
    "\n",
    "def build_autoencoder(input_dim, layer_ratios, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(layer_ratios[0] * input_dim), activation=activation, input_shape=(input_dim,)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(int(layer_ratios[1] * input_dim), activation=activation))\n",
    "    model.add(Dense(int(layer_ratios[2] * input_dim), activation='linear'))  # Bottleneck layer\n",
    "    model.add(Dense(int(layer_ratios[1] * input_dim), activation=activation))\n",
    "    model.add(Dense(int(layer_ratios[0] * input_dim), activation=activation))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(input_dim, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_autoencoder(data, model, epochs=50, batch_size=256):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model.fit(data, data, epochs=epochs, batch_size=batch_size, \n",
    "              validation_split=0.1, callbacks=[early_stopping], verbose=0)\n",
    "    return model\n",
    "\n",
    "def get_feature_importance(model, data):\n",
    "    predictions = model.predict(data)\n",
    "    reconstruction_error = np.mean((predictions - data) ** 2, axis=0)\n",
    "    return reconstruction_error\n",
    "\n",
    "def feature_selection(dev_F, dev_NF, oos_F, oos_NF, feature_threshold, ratios, activation):\n",
    "    # Train on fraud examples\n",
    "    model_F = build_autoencoder(dev_F.shape[1], ratios, activation)\n",
    "    model_F = train_autoencoder(dev_F, model_F)\n",
    "    importance_F = get_feature_importance(model_F, dev_F)\n",
    "    \n",
    "    # Train on non-fraud examples\n",
    "    model_NF = build_autoencoder(dev_NF.shape[1], ratios, activation)\n",
    "    model_NF = train_autoencoder(dev_NF, model_NF)\n",
    "    importance_NF = get_feature_importance(model_NF, dev_NF)\n",
    "    \n",
    "    # Determine features to drop\n",
    "    features_to_drop = determine_features_to_drop(importance_F, importance_NF, feature_threshold)\n",
    "    return features_to_drop\n",
    "\n",
    "def determine_features_to_drop(importance_F, importance_NF, feature_threshold):\n",
    "    top_features_NF = np.argsort(importance_NF)[-int(len(importance_NF) * feature_threshold):]\n",
    "    bottom_features_F = np.where(importance_F <= 0)[0]\n",
    "    features_to_drop = np.union1d(top_features_NF, bottom_features_F)\n",
    "    return features_to_drop\n",
    "\n",
    "def drop_features(data, features_to_drop):\n",
    "    return data.drop(columns=features_to_drop)\n",
    "\n",
    "def train_final_autoencoder(data, ratios, activation):\n",
    "    model = build_autoencoder(data.shape[1], ratios, activation)\n",
    "    model = train_autoencoder(data, model)\n",
    "    return model\n",
    "\n",
    "def encode_data(model, data):\n",
    "    encoder = Sequential(model.layers[:4])  # Extract encoder part\n",
    "    encoded_data = encoder.predict(data)\n",
    "    return encoded_data\n",
    "\n",
    "def grid_search_logistic(X_train, y_train, X_val, y_val):\n",
    "    param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'saga']}\n",
    "    lr = LogisticRegression()\n",
    "    grid = GridSearchCV(lr, param_grid, cv=5, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    return best_model, f1\n",
    "\n",
    "def save_results(log_file, best_params, features_dropped, encoded_dev, encoded_oos, encoded_oot, encoder_model, logistic_model):\n",
    "    pd.DataFrame(log_file).to_csv('pipeline_log.csv', index=False)\n",
    "    with open('best_params.txt', 'w') as f:\n",
    "        f.write(str(best_params))\n",
    "    with open('features_dropped.txt', 'w') as f:\n",
    "        f.write(str(features_dropped))\n",
    "    pd.DataFrame(encoded_dev).to_csv('encoded_dev.csv', index=False)\n",
    "    pd.DataFrame(encoded_oos).to_csv('encoded_oos.csv', index=False)\n",
    "    pd.DataFrame(encoded_oot).to_csv('encoded_oot.csv', index=False)\n",
    "    encoder_model.save('encoder_model.h5')\n",
    "    joblib.dump(logistic_model, 'logistic_model.pkl')\n",
    "\n",
    "def pipeline(dev_path, oos_path, oot_path, hyperparameters):\n",
    "    dev, oos, oot = load_data(dev_path, oos_path, oot_path)\n",
    "    dev_scaled, oos_scaled, oot_scaled, scaler = standardize_data(dev, oos, oot)\n",
    "    dev_F, dev_NF, oos_F, oos_NF = split_data(dev_scaled, oos_scaled)\n",
    "    \n",
    "    features_to_drop = feature_selection(dev_F, dev_NF, oos_F, oos_NF, \n",
    "                                         hyperparameters['feature_threshold'], \n",
    "                                         hyperparameters['ratios'], \n",
    "                                         hyperparameters['activation'])\n",
    "    \n",
    "    new_dev = drop_features(dev_scaled, features_to_drop)\n",
    "    new_oos = drop_features(oos_scaled, features_to_drop)\n",
    "    new_oot = drop_features(oot_scaled, features_to_drop)\n",
    "    \n",
    "    final_autoencoder = train_final_autoencoder(new_dev, hyperparameters['ratios'], hyperparameters['activation'])\n",
    "    encoded_dev = encode_data(final_autoencoder, new_dev)\n",
    "    encoded_oos = encode_data(final_autoencoder, new_oos)\n",
    "    encoded_oot = encode_data(final_autoencoder, new_oot)\n",
    "    \n",
    "    best_logistic_model, best_f1 = grid_search_logistic(encoded_dev, dev['target'], encoded_oos, oos['target'])\n",
    "    \n",
    "    save_results(hyperparameters, best_logistic_model.get_params(), features_to_drop, \n",
    "                 encoded_dev, encoded_oos, encoded_oot, final_autoencoder, best_logistic_model)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    hyperparameters = {\n",
    "        'feature_threshold': 0.1,\n",
    "        'ratios': [0.8, 0.5, 0.2],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "    pipeline('dev.csv', 'oos.csv', 'oot.csv', hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
